{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Project: planning my next holidays ‚òÄÔ∏è\n",
    "\n",
    "Let's create a script that allows to get some information about all the hotels in a given city on <a href=\"https://www.booking.com\" target=\"_blank\">www.booking.com</a> üßô\n",
    "\n",
    "**We strongly recommend that you use Scrapy, it will be much easier!**\n",
    "\n",
    "You can scrap as many information as you want, but we suggest that you get at least:\n",
    "\n",
    "* The hotel name, \n",
    "* The url to its booking.com page, \n",
    "* Its coordinates: latitude and longitude,\n",
    "* The score given by the website users,\n",
    "* The text description of the hotel.\n",
    "\n",
    "Then, you can execute this script for several cities from yesterday's list. Make sure you save the results in different files for each city and that the name of the city is stored in the filename (for later purposes üòâ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /opt/conda/lib/python3.8/site-packages (2.5.1)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /opt/conda/lib/python3.8/site-packages (from scrapy) (4.6.3)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /opt/conda/lib/python3.8/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\" in /opt/conda/lib/python3.8/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /opt/conda/lib/python3.8/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from scrapy) (0.4.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: Twisted[http2]>=17.9.0 in /opt/conda/lib/python3.8/site-packages (from scrapy) (21.7.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.8/site-packages (from scrapy) (3.1.1)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.8/site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: h2<4.0,>=3.0 in /opt/conda/lib/python3.8/site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/conda/lib/python3.8/site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /opt/conda/lib/python3.8/site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /opt/conda/lib/python3.8/site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /opt/conda/lib/python3.8/site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: six>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.8/site-packages (from Twisted[http2]>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from Twisted[http2]>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.8/site-packages (from Twisted[http2]>=17.9.0->scrapy) (3.7.4.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from Twisted[http2]>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /opt/conda/lib/python3.8/site-packages (from Twisted[http2]>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.8/site-packages (from Twisted[http2]>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: priority<2.0,>=1.1.0; extra == \"http2\" in /opt/conda/lib/python3.8/site-packages (from Twisted[http2]>=17.9.0->scrapy) (1.3.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.14.3)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /opt/conda/lib/python3.8/site-packages (from h2<4.0,>=3.0->scrapy) (3.0.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /opt/conda/lib/python3.8/site-packages (from h2<4.0,>=3.0->scrapy) (5.2.0)\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from zope.interface>=4.1.3->scrapy) (49.6.0.post20201009)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.8/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/conda/lib/python3.8/site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (2.10)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_name = 'Annecy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hotels(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"hotels\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['https://www.booking.com/index.fr.html']\n",
    "\n",
    "    # Parse function for login\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to login\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'ss': destination_name},\n",
    "            callback=self.after_search\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_search(self, response):\n",
    "        \n",
    "        hotels = response.css('.sr_item')\n",
    "\n",
    "        for h in hotels:\n",
    "            yield {\n",
    "                'name': h.css('.sr-hotel__name::text').get(),\n",
    "                'url': \"https://www.booking.com\" + h.css('.hotel_name_link').attrib[\"href\"],\n",
    "                'coords': h.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                'score': h.css('.bui-review-score__badge::text').get(),\n",
    "                'description': h.css('.hotel_desc::text').get()\n",
    "                \n",
    "            }\n",
    "        \n",
    "        \n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 14:02:37 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2021-10-16 14:02:37 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-5.4.129+-x86_64-with-glibc2.10\n",
      "2021-10-16 14:02:37 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-10-16 14:02:37 [scrapy.extensions.telnet] INFO: Telnet Password: f5767e16f315870c\n",
      "2021-10-16 14:02:37 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-10-16 14:02:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-10-16 14:02:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-10-16 14:02:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-10-16 14:02:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-10-16 14:02:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-10-16 14:02:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-10-16 14:03:02 [root] INFO: No next page. Terminating crawling process.\n",
      "2021-10-16 14:03:02 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-10-16 14:03:02 [scrapy.extensions.feedexport] INFO: Stored json feed (515 items) in: /home/jovyan/Full stack Alexis/res/2_hotels_Annecy.json\n",
      "2021-10-16 14:03:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 36221,\n",
      " 'downloader/request_count': 22,\n",
      " 'downloader/request_method_count/GET': 22,\n",
      " 'downloader/response_bytes': 3628925,\n",
      " 'downloader/response_count': 22,\n",
      " 'downloader/response_status_count/200': 22,\n",
      " 'elapsed_time_seconds': 24.426194,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 10, 16, 14, 3, 2, 436746),\n",
      " 'httpcompression/response_bytes': 20379578,\n",
      " 'httpcompression/response_count': 22,\n",
      " 'item_scraped_count': 515,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 78950400,\n",
      " 'memusage/startup': 78950400,\n",
      " 'request_depth_max': 21,\n",
      " 'response_received_count': 22,\n",
      " 'scheduler/dequeued': 22,\n",
      " 'scheduler/dequeued/memory': 22,\n",
      " 'scheduler/enqueued': 22,\n",
      " 'scheduler/enqueued/memory': 22,\n",
      " 'start_time': datetime.datetime(2021, 10, 16, 14, 2, 38, 10552)}\n",
      "2021-10-16 14:03:02 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = \"2_hotels_\" + destination_name.replace(\" \", \"-\") + \".json\"\n",
    "\n",
    "if filename in os.listdir('/home/jovyan/Full stack Alexis/res/'):\n",
    "        os.remove('/home/jovyan/Full stack Alexis/res/' + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        '/home/jovyan/Full stack Alexis/res/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(Hotels)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
